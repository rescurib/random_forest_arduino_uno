# Implementaci√≥n de un clasificador de bosque aleatorio en un microcontrolador

En este respositorio:
1. Entrenamos un **Random Forest** con el *Wine Dataset* (UCI).  
2. Exportamos el modelo a **c√≥digo C** usando `m2cgen`, para poder ejecutarlo en microcontroladores (TinyML).  

Este flujo es un ejemplo de c√≥mo pasar de un modelo entrenado con Scikit-Learn (Python) a una implementaci√≥n que corre en hardware con recursos limitados. Por simplicidad, el modelo en C correr√° en el framwork de Arduino, pero la implementaci√≥n es porteable a cualquier otro framework o modelo de microcontrolador.

## Introducci√≥n

Los **bosques aleatorios** (*Random Forests*) son uno de los algoritmos m√°s populares y robustos del aprendizaje autom√°tico. Pueden usarse tanto en problemas de **clasificaci√≥n** como de **regresi√≥n** y se basan en la idea de combinar muchos **√°rboles de decisi√≥n** para obtener una predicci√≥n m√°s estable y precisa.

### üåø ¬øQu√© es un √Årbol de Decisi√≥n?

Un **√°rbol de decisi√≥n** es un modelo que representa decisiones y sus posibles consecuencias mediante una estructura en forma de √°rbol. Cada **nodo** corresponde a una pregunta sobre una caracter√≠stica, por ejemplo, en el caso de la clasificaci√≥n de vinos: "*¬øalcohol > 12.5*?". Cada **hoja** contiene la predicci√≥n final (por ejemplo, "Clase: Tipo 1").

```mermaid
graph TD
    A["¬øAlcohol > 12.5?"] -->|S√≠| B["¬øColor intensity > 5?"]
    A -->|No| C["Clase: Tipo 3"]
    B -->|S√≠| D["Clase: Tipo 1"]
    B -->|No| E["Clase: Tipo 2"]
```

Cada uno de los caminos desde la ra√≠z hasta una hoja representa una secuencia de decisiones que lleva a una clasificaci√≥n.

### üå≤ ¬øQu√© es un Bosque Aleatorio?

Un **bosque aleatorio** es un conjunto (*ensamble*) de muchos √°rboles de decisi√≥n entrenados con inicializaciones aleator√≠as. Las dos fuentes principales de aleatoriedad son:

- **Bootstrap (bagging):** cada √°rbol se entrena con una muestra aleatoria (con reemplazo) del conjunto de entrenamiento.  
- **Selecci√≥n aleatoria de caracter√≠sticas:** en cada divisi√≥n, el √°rbol considera √∫nicamente un subconjunto aleatorio de las caracter√≠sticas.

Esta combinaci√≥n produce modelos con **menos varianza** que un √∫nico √°rbol profundo y reduce el riesgo de **sobreajuste**.

```mermaid
graph LR
    subgraph "Bosque Aleatorio (ejemplo)"
      T1["üå≥ √Årbol 1"] --> V1["Predice: Tipo 1"]
      T2["üå≤ √Årbol 2"] --> V2["Predice: Tipo 2"]
      T3["üå¥ √Årbol 3"] --> V3["Predice: Tipo 1"]
    end

    V1 --> R["üßæ Votaci√≥n"]
    V2 --> R
    V3 --> R
    R --> F["‚úÖ Predicci√≥n final: Tipo 1 (mayor√≠a de votos)"]
```

---

## ‚öôÔ∏è Ventajas y Consideraciones

**Ventajas**
- Robusto al ruido y a valores at√≠picos.  
- Reduce el sobreajuste respecto a √°rboles individuales.  
- Puede manejar datos con caracter√≠sticas de distinta escala.  
- Permite obtener medidas de importancia de caracter√≠sticas.  
- F√°cil de usar con bibliotecas como **scikit-learn**.

**Consideraciones**
- Pierde algo de interpretabilidad frente a un √∫nico √°rbol (aunque a√∫n se pueden inspeccionar √°rboles individuales).  
- Modelos grandes pueden consumir m√°s memoria y tiempo; para TinyML es com√∫n **podar** o convertir el modelo a una versi√≥n optimizada en C/C++.



## üìö Referencias
- Breiman, L. (2001). *Random Forests*. Machine Learning.  
- Documentaci√≥n de scikit-learn: `RandomForestClassifier`.  
- m2cgen: generador de c√≥digo para modelos (√∫til para TinyML).
