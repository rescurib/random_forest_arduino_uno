# Implementaci√≥n de un clasificador de bosque aleatorio en un microcontrolador

En este respositorio:
1. Entrenamos un **Random Forest** con el [conjunto de datos *Wine*](https://archive.ics.uci.edu/dataset/109/wine).  
2. Exportamos el modelo a **c√≥digo C** usando `m2cgen`, para poder ejecutarlo en microcontroladores (TinyML).  

Este flujo es un ejemplo de c√≥mo pasar de un modelo entrenado con Scikit-Learn (Python) a una implementaci√≥n que corre en hardware con recursos limitados. Por simplicidad, el modelo en C correr√° en el framwork de Arduino, pero la implementaci√≥n es porteable a cualquier otro framework o modelo de microcontrolador; s√≥lo hay que usar la funci√≥n de clasificaci√≥n del header *random_forest_model.h* y archivo que contiene el c√≥digo del modelo *random_forest_model.c*.

**Clonar repo** ([Instalar git](https://git-scm.com/downloads/win) si no lo has hecho)
```Bash
git clone https://github.com/rescurib/random_forest_arduino_uno.git
```

**Crear y activar entorno virtual de Python** 
Abrir una ventana de comandos dentro de la carpeta del repo y ejecutar:
```Bash
# En Windows
setup_env.cmd

# En Linux
./setup_env.sh
```

## Introducci√≥n

Los **bosques aleatorios** (*Random Forests*) son uno de los algoritmos m√°s populares y robustos del aprendizaje autom√°tico. Pueden usarse tanto en problemas de **clasificaci√≥n** como de **regresi√≥n** y se basan en la idea de combinar muchos **√°rboles de decisi√≥n** para obtener una predicci√≥n m√°s estable y precisa.

### üåø ¬øQu√© es un √Årbol de Decisi√≥n?

Un **√°rbol de decisi√≥n** es un modelo que representa decisiones y sus posibles consecuencias mediante una estructura en forma de √°rbol. Cada **nodo** corresponde a una pregunta sobre una caracter√≠stica, por ejemplo, en el caso de la clasificaci√≥n de vinos: "*¬øalcohol > 12.5*?". Cada **hoja** contiene la predicci√≥n final (por ejemplo, "Clase: Tipo 1").

```mermaid
graph TD
    A["¬øAlcohol > 12.5?"] -->|S√≠| B["¬øColor intensity > 5?"]
    A -->|No| C["Clase: Tipo 3"]
    B -->|S√≠| D["Clase: Tipo 1"]
    B -->|No| E["Clase: Tipo 2"]
```

Cada uno de los caminos desde la ra√≠z hasta una hoja representa una secuencia de decisiones que lleva a una clasificaci√≥n.

### üå≤ ¬øQu√© es un Bosque Aleatorio?

Un **bosque aleatorio** es un conjunto (*ensamble*) de muchos √°rboles de decisi√≥n entrenados con inicializaciones aleator√≠as. Las dos fuentes principales de aleatoriedad son:

- **Bootstrap (bagging):** cada √°rbol se entrena con una muestra aleatoria (con reemplazo) del conjunto de entrenamiento.  
- **Selecci√≥n aleatoria de caracter√≠sticas:** en cada divisi√≥n, el √°rbol considera √∫nicamente un subconjunto aleatorio de las caracter√≠sticas.

Esta combinaci√≥n produce modelos con **menos varianza** que un √∫nico √°rbol profundo y reduce el riesgo de **sobreajuste**.

```mermaid
graph LR
    subgraph "Bosque Aleatorio (ejemplo)"
      T1["üå≥ √Årbol 1"] --> V1["Predice: Tipo 1"]
      T2["üå≤ √Årbol 2"] --> V2["Predice: Tipo 2"]
      T3["üå¥ √Årbol 3"] --> V3["Predice: Tipo 1"]
    end

    V1 --> R["üßæ Votaci√≥n"]
    V2 --> R
    V3 --> R
    R --> F["‚úÖ Predicci√≥n final: Tipo 1 (mayor√≠a de votos)"]
```

## ‚öôÔ∏è Ventajas y Consideraciones

**Ventajas**
- Robusto al ruido y a valores at√≠picos.  
- Reduce el sobreajuste respecto a √°rboles individuales.  
- Puede manejar datos con caracter√≠sticas de distinta escala.  
- Permite obtener medidas de importancia de caracter√≠sticas.  
- F√°cil de usar con bibliotecas como **scikit-learn**.

**Consideraciones**
- Pierde algo de interpretabilidad frente a un √∫nico √°rbol (aunque a√∫n se pueden inspeccionar √°rboles individuales).  
- Modelos grandes pueden consumir m√°s memoria y tiempo; para TinyML es com√∫n **podar** o convertir el modelo a una versi√≥n optimizada en C/C++.

## Creaci√≥n y entrenamiento del modelo

```Python
from sklearn.datasets import load_wine
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Cargar el dataset Wine
data = load_wine()
X = data.data
y = data.target

# Dividir en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
        )

# Crear y entrenar el modelo Random Forest
clf = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)
clf.fit(X_train, y_train)

# Evaluar el modelo
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"Exactitud del modelo: {acc:.2f}")
print("N√∫mero de √°rboles entrenados:", len(clf.estimators_))
```

**Salida**
```Bash
Exactitud del modelo: 0.97
N√∫mero de √°rboles entrenados: 10
```

## Exportar modelo a c√≥digo en C

```Python
import m2cgen as m2c

# Generar c√≥digo C a partir del modelo
c_code = m2c.export_to_c(clf)

# Guardar el c√≥digo generado en un archivo .c
with open("random_forest_model.c", "w") as f:
    f.write("/*\n")
    f.write(" * Random Forest generado con m2cgen\n")
    f.write(f" * Exactitud en test: {acc:.2f}\n")
    f.write(" */\n\n")
    f.write(c_code)
```

La funci√≥n `score()` es el punto de entrada para realizar la inferencia con el modelo Random Forest exportado a C. Recibe un arreglo de 13 valores tipo `double` (las caracter√≠sticas de entrada) y devuelve en un arreglo de 3 valores las probabilidades para cada clase. Hay que tener cuidado porque la implementaci√≥n no verifica tama√±os de arreglos.

**Ejemplo de uso en C:**

```c
double features[13] = { /* tus valores de entrada */ };
double output[3];
score(features, output);
// output[0], output[1], output[2] contienen la probabilidad de cada clase
```

## Ejecuci√≥n del modelo en Arduino

El programa en Arduino espera recibir por el puerto serie una l√≠nea con 13 caracter√≠sticas num√©ricas (del dataset Wine), realiza la inferencia usando el modelo Random Forest exportado a C y responde por el mismo puerto la clase predicha y el tiempo de inferencia.

### Esquema general del flujo

```mermaid
flowchart TD
    A["Datos de entrada por Serial (13 caracter√≠sticas, inicia con '$')"]
    B["Parseo y validaci√≥n de datos"]
    C["Inferencia (Random Forest en C)"]
    D["Selecci√≥n de clase m√°s probable"]
    E["Env√≠o de resultado por Serial (Clase y tiempo de inferencia)"]
    F["Error de lectura (Respuesta de error por Serial)"]

    A --> B
    B -- "Datos v√°lidos" --> C
    B -- "Datos inv√°lidos" --> F
    C --> D
    D --> E
```

### Detalles del protocolo de comunicaci√≥n

**Entrada:**

- Formato:
    ```text
    $<f1>,<f2>,...,<f13>\n
    # 13 valores tipo float/double separados por coma, inicia con '$'
    ```
- Ejemplo:
    ```text
    $13.2,2.7,2.5,15.6,98.0,2.1,2.0,0.3,1.6,5.1,1.1,3.0,1065.0\n
    # Cada n√∫mero representa una caracter√≠stica del dataset Wine
    ```

**Salida:**

- Si la inferencia es exitosa:
    ```text
    Clase m√°s probable: <clase>
    Tiempo de inferencia (us): <tiempo>
    ```

Si compiamos y pegamos esta muestra en el monitor serial del IDE de Arduino este es el resultado:
```Bash
Clase m√°s probable: 1
Tiempo de inferencia (us): 428
```

## Programa de env√≠o de muestra

Una vez que hayas cargado el programa en el Arduino, ajusta el n√∫mero de puerto COM en `clasificador_serial_arduino.py`. Este programa selecciona un elemento aleatorio del conjunto Wine, lo env√≠a al Arduino por serial y evalua su respuesta.
Para ejecutarlo:
```Bash
python clasificador_serial_arduino.py

# En Windows tal vez tengas que usar:
py clasificador_serial_arduino.py
```